{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>artist</th>\n",
       "      <th>title</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>genre</th>\n",
       "      <th>mood</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>TRBIGRY128F42597B3.h5</td>\n",
       "      <td>Sade</td>\n",
       "      <td>All About Our Love</td>\n",
       "      <td>Its all about our love\\nSo shall it be forever...</td>\n",
       "      <td>R&amp;B</td>\n",
       "      <td>sad</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>TRBIIEU128F9307C88.h5</td>\n",
       "      <td>New Found Glory</td>\n",
       "      <td>Don't Let Her Pull You Down</td>\n",
       "      <td>It's time that I rain on your parade\\nWatch as...</td>\n",
       "      <td>Rock</td>\n",
       "      <td>happy</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997</td>\n",
       "      <td>TRBIIJY12903CE4755.h5</td>\n",
       "      <td>Mindy McCready</td>\n",
       "      <td>Ten Thousand Angels</td>\n",
       "      <td>Speakin of the devil\\nLook who just walked in\\...</td>\n",
       "      <td>Country</td>\n",
       "      <td>happy</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>TRBIIOT128F423C594.h5</td>\n",
       "      <td>Joy Division</td>\n",
       "      <td>Leaders Of Men</td>\n",
       "      <td>Born from some mother's womb\\nJust like any ot...</td>\n",
       "      <td>Rock</td>\n",
       "      <td>sad</td>\n",
       "      <td>1978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>999</td>\n",
       "      <td>TRBIJYB128F14AE326.h5</td>\n",
       "      <td>Seventh Day Slumber</td>\n",
       "      <td>Shattered Life</td>\n",
       "      <td>This wanting more from me is tearing me, it's ...</td>\n",
       "      <td>Rock</td>\n",
       "      <td>sad</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      file               artist                        title  \\\n",
       "995  TRBIGRY128F42597B3.h5                 Sade           All About Our Love   \n",
       "996  TRBIIEU128F9307C88.h5      New Found Glory  Don't Let Her Pull You Down   \n",
       "997  TRBIIJY12903CE4755.h5       Mindy McCready          Ten Thousand Angels   \n",
       "998  TRBIIOT128F423C594.h5         Joy Division               Leaders Of Men   \n",
       "999  TRBIJYB128F14AE326.h5  Seventh Day Slumber               Shattered Life   \n",
       "\n",
       "                                                lyrics    genre   mood  year  \n",
       "995  Its all about our love\\nSo shall it be forever...      R&B    sad  2000  \n",
       "996  It's time that I rain on your parade\\nWatch as...     Rock  happy  2009  \n",
       "997  Speakin of the devil\\nLook who just walked in\\...  Country  happy  1996  \n",
       "998  Born from some mother's womb\\nJust like any ot...     Rock    sad  1978  \n",
       "999  This wanting more from me is tearing me, it's ...     Rock    sad  2005  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('train_lyrics_1000.csv')\n",
    "df_test = pd.read_csv('valid_lyrics_200.csv')\n",
    "\n",
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>artist</th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>mood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>TRAFAIB128F426E636.h5</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>Where Is My Mind (XFM Live Version)</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Ooooohh\\nOoooohh\\nOoooohh\\nOoooohh\\n\\nWith you...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>TRAFAJC128E078888B.h5</td>\n",
       "      <td>Queens Of The Stone Age</td>\n",
       "      <td>This Lullaby</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Where, oh, where have you been, my love?\\nWher...</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>TRAFBBP128F92F6CC9.h5</td>\n",
       "      <td>MC5</td>\n",
       "      <td>Looking At You (Cody High School)</td>\n",
       "      <td>Rock</td>\n",
       "      <td>When it happened \\nsomething snapped inside \\n...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>TRAFBVU128F426B3F6.h5</td>\n",
       "      <td>Dimmu Borgir</td>\n",
       "      <td>The Fundamental Alienation</td>\n",
       "      <td>Rock</td>\n",
       "      <td>My eyes got blinded\\nAnd conned by the light\\n...</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>TRAFEEO12903CFEC87.h5</td>\n",
       "      <td>Dark Fortress</td>\n",
       "      <td>The Silver Gate</td>\n",
       "      <td>Rock</td>\n",
       "      <td>[Music: V Santura, Lyrics: Morean]\\n\\nAwake, a...</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    file                   artist  \\\n",
       "0  TRAFAIB128F426E636.h5                  Placebo   \n",
       "1  TRAFAJC128E078888B.h5  Queens Of The Stone Age   \n",
       "2  TRAFBBP128F92F6CC9.h5                      MC5   \n",
       "3  TRAFBVU128F426B3F6.h5             Dimmu Borgir   \n",
       "4  TRAFEEO12903CFEC87.h5            Dark Fortress   \n",
       "\n",
       "                                 title genre  \\\n",
       "0  Where Is My Mind (XFM Live Version)   Pop   \n",
       "1                         This Lullaby  Rock   \n",
       "2    Looking At You (Cody High School)  Rock   \n",
       "3           The Fundamental Alienation  Rock   \n",
       "4                      The Silver Gate  Rock   \n",
       "\n",
       "                                              lyrics   mood  \n",
       "0  Ooooohh\\nOoooohh\\nOoooohh\\nOoooohh\\n\\nWith you...  happy  \n",
       "1  Where, oh, where have you been, my love?\\nWher...    sad  \n",
       "2  When it happened \\nsomething snapped inside \\n...  happy  \n",
       "3  My eyes got blinded\\nAnd conned by the light\\n...    sad  \n",
       "4  [Music: V Santura, Lyrics: Morean]\\n\\nAwake, a...    sad  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'happy', 'sad'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_test[\"mood\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train['lyrics']\n",
    "y_train = df_train['mood']\n",
    "\n",
    "X_test = df_test['lyrics']\n",
    "y_test = df_test['mood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 0      sad\n",
      "1    happy\n",
      "2      sad\n",
      "3    happy\n",
      "4      sad\n",
      "Name: mood, dtype: object ...\n",
      "after: [1 0 1 0 1] ...\n"
     ]
    }
   ],
   "source": [
    "# Label encoder\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "pickle_in = open('./label_encoder.p', 'rb')\n",
    "le = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "print('before: %s ...' %y_train[:5])\n",
    "\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "print('after: %s ...' %y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "stop_words = pickle.load(open('./stopwords.p', 'rb'))\n",
    "semantic_words = pickle.load(open('./whitelist_dicts/semantic_words_py34.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform texts into bag of words models - Trying different tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = EnglishStemmer()\n",
    "\n",
    "# raw words\n",
    "tokenizer = lambda text: text.split()\n",
    "\n",
    "# words after Porter stemming \n",
    "tokenizer_porter = lambda text: [porter.stem(word) for word in text.split()]\n",
    "\n",
    "# Words after Snowball stemming\n",
    "tokenizer_snowball = lambda text: [snowball.stem(word) for word in text.split()]\n",
    "\n",
    "# Only words that are in a list of 'positive' or 'negative' words ('whitelist')\n",
    "# http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon\n",
    "tokenizer_whitelist = lambda text: [word for word in text.split() if word in semantic_words]\n",
    "\n",
    "# Porter-stemmed words in whitelist\n",
    "tokenizer_porter_wl = lambda text: [porter.stem(word) for word in text.split() if word in semantic_words]\n",
    "\n",
    "# Snowball-stemmed words in whitelist\n",
    "tokenizer_snowball_wl = lambda text: [snowball.stem(word) for word in text.split() if word in semantic_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at vocabulary sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import mlxtend\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.preprocessing import DenseTransformer\n",
    "\n",
    "vect_1 = CountVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer)\n",
    "\n",
    "vect_2 = CountVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer_porter)\n",
    "    \n",
    "vect_3 = CountVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer_snowball)  \n",
    "\n",
    "vect_4 = CountVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer_whitelist)  \n",
    "\n",
    "vect_5 = CountVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer_porter_wl)\n",
    "\n",
    "vect_6 = CountVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer_snowball_wl)\n",
    "\n",
    "vect_7 = TfidfVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer)\n",
    "\n",
    "vect_8 = TfidfVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer_porter)\n",
    "    \n",
    "vect_9 = TfidfVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer_snowball)\n",
    "\n",
    "vect_10 = TfidfVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer_whitelist)    \n",
    "\n",
    "vect_11 = TfidfVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer_porter_wl)\n",
    "\n",
    "vect_12 = TfidfVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer_snowball_wl)\n",
    "\n",
    "\n",
    "pipelines = []\n",
    "vectorizers = [vect_1, vect_2, vect_3, vect_4, vect_5, vect_6, vect_7, vect_8, vect_9, vect_10, vect_11, vect_12]\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "for v in vectorizers:\n",
    "    pipelines.append(Pipeline([('vect', v),\n",
    "                               ('dense', DenseTransformer()),\n",
    "                               ('clf',RandomForestClassifier())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary sizes\n",
      "\n",
      "CountVec: 11378\n",
      "CountVec porter: 8551\n",
      "CountVec snowball: 8528\n",
      "CountVec wl: 1666\n",
      "CountVec porter+wl: 1349\n",
      "CountVec snowball+wl: 1332\n",
      "TfidfVec: 11378\n",
      "TfidfVec porter: 8551\n",
      "TfidfVec snowball: 8528\n",
      "TfidfVec wl: 1666\n",
      "TfidfVec porter+wl: 1349\n",
      "TfidfVec snowball+wl: 1332\n"
     ]
    }
   ],
   "source": [
    "# done before max_features was set\n",
    "\n",
    "print('Vocabulary sizes\\n')\n",
    "labels = ['CountVec', 'CountVec porter', 'CountVec snowball', 'CountVec wl', 'CountVec porter+wl','CountVec snowball+wl',\n",
    "          'TfidfVec', 'TfidfVec porter', 'TfidfVec snowball', 'TfidfVec wl', 'TfidfVec porter+wl','TfidfVec snowball+wl',]\n",
    "\n",
    "for label, v in zip(labels, vectorizers):\n",
    "    v.fit(X_train)\n",
    "    print('%s: %s' % (label, len(v.vocabulary_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction - Cross Validation Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf 1, CountVec: 72.01779401154401\n",
      "clf 2, CountVec porter: 73.72390873015873\n",
      "clf 3, CountVec snowball: 73.2554743867244\n",
      "clf 4, CountVec wl: 68.14885461760463\n",
      "clf 5, CountVec porter+wl: 69.3327380952381\n",
      "clf 6, CountVec snowball+wl: 68.55391414141414\n",
      "clf 7, TfidfVec: 70.22784992784995\n",
      "clf 8, TfidfVec porter: 74.54566197691197\n",
      "clf 9, TfidfVec snowball: 72.85374278499278\n",
      "clf 10, TfidfVec wl: 67.15810786435787\n",
      "clf 11, TfidfVec porter+wl: 67.82792207792208\n",
      "clf 12, TfidfVec snowball+wl: 68.72653318903319\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "labels = ['CountVec', 'CountVec porter', 'CountVec snowball', 'CountVec wl', 'CountVec porter+wl','CountVec snowball+wl',\n",
    "          'TfidfVec', 'TfidfVec porter', 'TfidfVec snowball', 'TfidfVec wl', 'TfidfVec porter+wl','TfidfVec snowball+wl',]\n",
    "\n",
    "\n",
    "\n",
    "d = {'Data':labels,\n",
    "     'ROC AUC (%)':[],}\n",
    "\n",
    "for i,clf in enumerate(pipelines):\n",
    "    scores = cross_val_score(estimator=clf, X=X_train, y=y_train, scoring='roc_auc', cv=10)\n",
    "    print('clf %s, %s: %s' % (i+1, labels[i], scores.mean()*100))\n",
    "    d['ROC AUC (%)'].append('%0.2f (+/- %0.2f)' % (scores.mean()*100, scores.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "y_predict=cross_val_predict(clf,X_test,y_test)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your selected lyrics : Where, oh, where have you been, my love?\n",
      "Where, oh, where can you be?\n",
      "It's been so long since the moon has gone\n",
      "And, oh, what a wreck you've made me\n",
      "\n",
      "Are you there over the ocean?\n",
      "Are you there up in the sky?\n",
      "Until the return of my love\n",
      "This lullaby\n",
      "\n",
      "My hope is on the horizon\n",
      "Every face, your eyes I can see\n",
      "I plead and I pray through each night and day\n",
      "Our embrace is only a dream\n",
      "\n",
      "And as sure as days come from moments\n",
      "Each hour becomes a life's time\n",
      "When she'd left I'd only begun\n",
      "This lullaby\n",
      "\r\n",
      "\r\n",
      "model prediction : happy\n"
     ]
    }
   ],
   "source": [
    "d={0:\"happy\",1:\"sad\"}\n",
    "print(\"your selected lyrics :\",X_test[1])\n",
    "print(\"\\r\")\n",
    "print(\"\\r\")\n",
    "print(\"model prediction :\",d[y_predict[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score of our model : 0.63\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"accuracy_score of our model :\",accuracy_score(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1, 'Confusion matrix')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "c=confusion_matrix(y_test,y_predict)\n",
    "sns.heatmap(c,annot=True,fmt='d')\n",
    "plt.title(\"Confusion matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_perform.to_csv('./random_forests_data/rand_forest_featextr_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
